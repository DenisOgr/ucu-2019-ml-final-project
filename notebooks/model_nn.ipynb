{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "wVYAgEXBvbTt",
    "outputId": "b6b07ab8-5cb0-4c4e-a1cb-41733a96d592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#Google Colab needs\n",
    "import os\n",
    "import sys\n",
    "def isCollab():\n",
    "    return os.environ.get('COLAB_GPU', None) != None\n",
    "\n",
    "if isCollab():\n",
    "    #Mounting GDrive disc\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    path_to_storage = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "\n",
    "    #Append path where custom modules stored. I put custom modules to GDrive disc\n",
    "    path_to_modules = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "    sys.path.append(path_to_modules)\n",
    "else:\n",
    "    sys.path.append('..')\n",
    "    path_to_storage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hNJoM-rfwvOS",
    "outputId": "d93bc52e-1e5b-40b4-e516-820501d17430"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Input\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Cd82tzphXCLI",
    "outputId": "d9a9b8be-fb92-4c93-e97b-068c74c22724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvEOuuv6BkId"
   },
   "outputs": [],
   "source": [
    "if not path_to_storage:\n",
    "    path_to_storage = os.path.abspath(os.path.join(os.getcwd(), '../storage')) \n",
    "\n",
    "path_to_objects = path_to_storage + \"/serialization_objects/\"\n",
    "data_folder = path_to_storage+'/data/'\n",
    "X_train = pickle.load(open(path_to_objects + 'X_train.p', 'rb'))\n",
    "y_train = pickle.load(open(path_to_objects + 'y_train.p', 'rb'))\n",
    "X_test = pickle.load(open(path_to_objects + 'X_test.p', 'rb'))\n",
    "y_test = pickle.load(open(path_to_objects + 'y_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbQBEKenVRa6"
   },
   "outputs": [],
   "source": [
    "q1_vecs_train = pickle.load(open(path_to_objects + 'X_train_q1_w2v_vect.p', 'rb'))\n",
    "q2_vecs_train = pickle.load(open(path_to_objects + 'X_train_q2_w2v_vect.p', 'rb'))\n",
    "q1_vecs_test = pickle.load(open(path_to_objects + 'X_test_q1_w2v_vect.p', 'rb'))\n",
    "q2_vecs_test = pickle.load(open(path_to_objects + 'X_test_q2_w2v_vect.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['question1'] = q1_vecs_train\n",
    "X_train['question2'] = q2_vecs_train\n",
    "X_test['question1'] = q1_vecs_test\n",
    "X_test['question2'] = q2_vecs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF3NJWxAW0PI"
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' \n",
    "    Pre process and convert texts to a list of words \n",
    "    input: str\n",
    "    output: list of cleaned word\n",
    "    '''\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(\"quikly\",\"quickly\", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uuVtfhUeWhCe",
    "outputId": "de0b3508-510c-4f8f-fb78-3a80e63447a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "questions_cols = ['question1', 'question2']\n",
    "filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [X_train, X_test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in preprocess_string(row[question], filters):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions with lists of word indices\n",
    "            dataset.set_value(index, question, q2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0I5RNGyRHPT"
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(X_train.question1.map(lambda x:len(x)).max(),\n",
    "                    X_train.question2.map(lambda x:len(x)).max(),\n",
    "                    X_test.question1.map(lambda x:len(x)).max(),\n",
    "                    X_test.question2.map(lambda x:len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U_vwLYXdRVih",
    "outputId": "6f38b86c-236d-44a6-a1e4-61b4d18efc1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182150"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrDdQOvm57ok"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sbbz8hfGXuP9"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  \n",
    "embeddings[0] = 0 \n",
    "\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60MYePK3cHeX"
   },
   "outputs": [],
   "source": [
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "iwVKQc186CEb",
    "outputId": "96fa9075-fe23-4426-9cea-f5e6b7f7cdbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0713 08:30:39.199456 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0713 08:30:39.244539 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0713 08:30:39.257477 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0713 08:30:39.275491 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0713 08:30:39.276655 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "from keras.layers import Input, Bidirectional, LSTM, dot, Flatten, Dense, Reshape, add, Dropout, BatchNormalization\n",
    "  \n",
    "n_hidden = 50\n",
    "gradient_clipping_norm  = 1.25\n",
    "\n",
    "batch_size = 64\n",
    "n_epoch = 6\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = layers.Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "shared_lstm = keras.layers.LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "distance = keras.layers.Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "\n",
    "model = Model([left_input, right_input], [distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aIC-jGqFYXAt",
    "outputId": "c9db1cd6-f4f8-452b-ef60-d572397f39ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 08:30:40.646041 140058315331456 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQ4yeSzaZH9u"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "# Split to train validation\n",
    "validation_size = 40000\n",
    "training_size = len(X_train) - validation_size\n",
    "\n",
    "X = X_train[questions_cols]\n",
    "Y = y_train\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "X_train['left'] = pad_sequences(X_train['left'],  maxlen=max_seq_length)\n",
    "X_train['right'] = pad_sequences(X_train['right'],  maxlen=max_seq_length)\n",
    "X_validation['left'] = pad_sequences(X_validation['left'],  maxlen=max_seq_length)\n",
    "X_validation['right'] = pad_sequences(X_validation['right'],  maxlen=max_seq_length)\n",
    "X_test['left'] = pad_sequences(X_test['left'],  maxlen=max_seq_length)\n",
    "X_test['right'] = pad_sequences(X_test['right'],  maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YJQyqSp0VbtK",
    "outputId": "053492ce-261e-4f12-81b0-4c6e6a449e2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230872"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train['left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "VSupc6JHY3v4",
    "outputId": "d167c370-b312-4e43-c638-d53e0153c6e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 08:30:45.242915 140058315331456 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 230872 samples, validate on 40000 samples\n",
      "Epoch 1/6\n",
      "230872/230872 [==============================] - 1240s 5ms/step - loss: 0.2100 - acc: 0.7220 - val_loss: 0.1760 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.74700, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-01-0.75.hdf5\n",
      "Epoch 2/6\n",
      "230872/230872 [==============================] - 1205s 5ms/step - loss: 0.1645 - acc: 0.7655 - val_loss: 0.1633 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.74700 to 0.76850, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-02-0.77.hdf5\n",
      "Epoch 3/6\n",
      "230872/230872 [==============================] - 1206s 5ms/step - loss: 0.1525 - acc: 0.7854 - val_loss: 0.1585 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76850 to 0.77833, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-03-0.78.hdf5\n",
      "Epoch 4/6\n",
      "230872/230872 [==============================] - 1210s 5ms/step - loss: 0.1449 - acc: 0.7977 - val_loss: 0.1562 - val_acc: 0.7816\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77833 to 0.78163, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-04-0.78.hdf5\n",
      "Epoch 5/6\n",
      "230872/230872 [==============================] - 1210s 5ms/step - loss: 0.1396 - acc: 0.8074 - val_loss: 0.1546 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78163 to 0.78400, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-05-0.78.hdf5\n",
      "Epoch 6/6\n",
      "230872/230872 [==============================] - 1192s 5ms/step - loss: 0.1351 - acc: 0.8138 - val_loss: 0.1539 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78400 to 0.78545, saving model to /content/gdrive/My Drive/UCU-2019-final-project-storage/serialization_objects/weights-improvement-06-0.79.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61743e3208>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = path_to_objects + \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, epochs=n_epoch, callbacks=callbacks_list,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8_5M_UhzNsx3",
    "outputId": "05f380d3-ab9f-47c2-ca08-e1211d633d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "import h5py\n",
    "model.save(path_to_objects+\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6KTS6-Fu9yU"
   },
   "outputs": [],
   "source": [
    "model.save_weights(path_to_objects+\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuDYGldPVViI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I71uNG2_Vd6Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of model_nn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
