{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wVYAgEXBvbTt",
    "outputId": "04e334fa-7ed9-4980-b07f-9061a5ee6fec"
   },
   "outputs": [],
   "source": [
    "#in: build X with utils/function build_X('test|train')\n",
    "#out: metrics(acc, recall, prec), roc_curve, log_loss\n",
    "#Google Colab needs\n",
    "import os\n",
    "import sys\n",
    "def isCollab():\n",
    "    return os.environ.get('COLAB_GPU', None) != None\n",
    "\n",
    "if isCollab():\n",
    "    #Mounting GDrive disc\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_storage = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "\n",
    "    #Append path where custom modules stored. I put custom modules to GDrive disc\n",
    "    path_to_modules = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "    sys.path.append(path_to_modules)\n",
    "else:\n",
    "    sys.path.append('..')\n",
    "    path_to_storage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNJoM-rfwvOS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/denisporplenko/anaconda3/envs/ucu-2019-ml-final-project/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Input\n",
    "import gensim\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Cd82tzphXCLI",
    "outputId": "a5039d2c-08b3-4851-8326-ca2c8b7e993c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/denisporplenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvEOuuv6BkId"
   },
   "outputs": [],
   "source": [
    "if not path_to_storage:\n",
    "    path_to_storage = os.path.abspath(os.path.join(os.getcwd(), '../storage')) \n",
    "\n",
    "path_to_objects = path_to_storage + \"/serialization_objects/\"\n",
    "data_folder = path_to_storage+'/data/'\n",
    "X_train = pickle.load(open(path_to_objects + 'X_train.p', 'rb'))\n",
    "y_train = pickle.load(open(path_to_objects + 'y_train.p', 'rb'))\n",
    "X_test = pickle.load(open(path_to_objects + 'X_test.p', 'rb'))\n",
    "y_test = pickle.load(open(path_to_objects + 'y_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0I5RNGyRHPT"
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(X_train.question1.map(lambda x:len(x)).max(),\n",
    "                    X_train.question2.map(lambda x:len(x)).max(),\n",
    "                    X_test.question1.map(lambda x:len(x)).max(),\n",
    "                    X_test.question2.map(lambda x:len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "dbQBEKenVRa6",
    "outputId": "5dba484b-dbfd-4f20-b637-8f4ece21d473"
   },
   "outputs": [],
   "source": [
    "#Download GoogleNews-vectors-negative300.bin\n",
    "path_to_google_news_model = data_folder+'GoogleNews-vectors-negative300.bin'\n",
    "if not os.path.isfile(path_to_google_news_model):\n",
    "    !wget -P \"$data_folder\" -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_l6rNb2TVOUc"
   },
   "outputs": [],
   "source": [
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  \n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(path_to_google_news_model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF3NJWxAW0PI"
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' \n",
    "    Pre process and convert texts to a list of words \n",
    "    input: str\n",
    "    output: list of cleaned word\n",
    "    '''\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(\"quikly\",\"quickly\", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "uuVtfhUeWhCe",
    "outputId": "41c71b03-2d0f-4121-e292-f94931bf5fb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [X_train, X_test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions with lists of word indices\n",
    "            dataset.set_value(index, question, q2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sbbz8hfGXuP9"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  \n",
    "embeddings[0] = 0 \n",
    "\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60MYePK3cHeX"
   },
   "outputs": [],
   "source": [
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwVKQc186CEb"
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "  \n",
    "n_hidden = 50\n",
    "gradient_clipping_norm  = 1.25\n",
    "\n",
    "batch_size = 64\n",
    "n_epoch = 3\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = layers.Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "shared_lstm = keras.layers.LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "distance = keras.layers.Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "\n",
    "model = Model([left_input, right_input], [distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIC-jGqFYXAt"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQ4yeSzaZH9u"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "# Split to train validation\n",
    "validation_size = 40000\n",
    "training_size = len(X_train) - validation_size\n",
    "\n",
    "X = X_train[questions_cols]\n",
    "Y = y_train\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side],  maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "VSupc6JHY3v4",
    "outputId": "c9eba18b-017f-4202-c0a8-77c3e3740fe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 230872 samples, validate on 40000 samples\n",
      "Epoch 1/3\n",
      "230872/230872 [==============================] - 11446s 50ms/step - loss: 0.1806 - acc: 0.7353 - val_loss: 0.1643 - val_acc: 0.7633\n",
      "Epoch 2/3\n",
      "230872/230872 [==============================] - 11341s 49ms/step - loss: 0.1588 - acc: 0.7743 - val_loss: 0.1545 - val_acc: 0.7830\n",
      "Epoch 3/3\n",
      "230872/230872 [==============================] - 11317s 49ms/step - loss: 0.1506 - acc: 0.7893 - val_loss: 0.1498 - val_acc: 0.7890\n"
     ]
    }
   ],
   "source": [
    "trained = model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6KTS6-Fu9yU"
   },
   "outputs": [],
   "source": [
    "pickle.dump(trained, open(path_to_objects+\"lstm_trained.p\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_nn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ucu-2019-ml-final-project",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
