{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denisporplenko/anaconda3/envs/ucu-2019-ml-final-project/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel, rbf_kernel\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "from fastdtw import fastdtw\n",
    "import similaritymeasures\n",
    "from scipy.spatial import procrustes\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.func.functions import pickle_and_remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_to_storage = os.path.abspath(os.path.join(os.getcwd(), '../storage'))\n",
    "\n",
    "### For Google colab (chage dir from local to GDrive)\n",
    "### Mount gdrive and set path to folder\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# path_to_storage = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "\n",
    "data_folder = path_to_storage+'/data/'\n",
    "serialization_objects_folder = path_to_storage+'/serialization_objects/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]\n",
    "def tokenize(data_type='train'):\n",
    "    if data_type=='test':\n",
    "        X = pickle.load(open(serialization_objects_folder+'X_test.p', 'rb'))\n",
    "    else:\n",
    "        X = pickle.load(open(serialization_objects_folder+'X_train.p', 'rb'))\n",
    "    series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str)\n",
    "    series.dropna()\n",
    "    for question in series:\n",
    "        yield preprocess_string(question, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_model(model, data_type):\n",
    "    \"\"\"\n",
    "    This function tokenize and check token for dict in model\n",
    "    :param:  model, data_type\n",
    "    :return: ndarray \n",
    "    \"\"\"\n",
    "    for question in tokenize(data_type=data_type):\n",
    "        tf_idf_tokens = []\n",
    "        for token in question:\n",
    "            try:\n",
    "                vector = model.wv[token]\n",
    "                tf_idf_tokens.append(token)\n",
    "            except:\n",
    "                continue\n",
    "        yield np.array(tf_idf_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding: word2vec (Transfer-train on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = [question for question in tokenize(data_type='train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec(tokenized_questions, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.intersect_word2vec_format(data_folder+'GoogleNews-vectors-negative300.bin',\n",
    "                                lockf=1.0,\n",
    "                                binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28526660, 35550940)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(tokenized_questions,total_examples=model_w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Sets 1 - Pairwise Distance & word2vec Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - TF-IDF Vectorizer and TF-IDF Weights + word2vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(serialization_objects_folder+'X_train.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "X_tfidf_all_q = tfidf.fit_transform(tokenize_for_model(model=model_w2v,data_type='train'))\n",
    "X_q1_tfidf = X_tfidf_all_q[:len(X_train)]\n",
    "X_q2_tfidf = X_tfidf_all_q[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270872, 31889),\n",
       " (270872, 31889),\n",
       " (270872, 4),\n",
       " matrix([[0.56637731, 0.66474651, 0.14063621, 0.46642286]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X1_q1_tfidf[0] - sparsed vector with float (tfidf)\n",
    "X_q1_tfidf.shape, X_q2_tfidf.shape, X_train.shape, X_q1_tfidf[0,X_q1_tfidf[0].todense().nonzero()[1]].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute TF-IDF weights as well as the word2vec vectors for all tokens\n",
    "def get_weights_and_w2vectors(tfidf_matrix, tfidf_vectorizer, model):\n",
    "    weights = []\n",
    "    vectors = []\n",
    "    rows = tfidf_matrix.shape[0]\n",
    "    inverse_vocab_dict = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}\n",
    "    for doc in range(rows):\n",
    "        features = tfidf_matrix[doc,:].nonzero()[1]\n",
    "        #weights[i] - all tfidf value for i- row (len(w[i]) - number token/words in row/question)\n",
    "        weights.append(np.array([tfidf_matrix[doc, x] for x in features]))\n",
    "        #vectors[i] - all vectors embeded from model. (len(w[i]) - number token/words in row/question)\n",
    "        vectors.append(np.array([model.wv[inverse_vocab_dict[x]] for x in features]))\n",
    "    return np.array(weights), np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q1_tfidf, X_q1_w2v_vect = get_weights_and_w2vectors(X_q1_tfidf, tfidf, model_w2v)\n",
    "X_q2_tfidf, X_q2_w2v_vect = get_weights_and_w2vectors(X_q2_tfidf, tfidf, model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "X_q1_tfidf[0].shape, X_q1_w2v_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_q1_tfidf, open(serialization_objects_folder+'X_train_q1_tfidf.p','wb'))\n",
    "pickle.dump(X_q2_tfidf, open(serialization_objects_folder+'X_train_q2_tfidf.p','wb'))\n",
    "pickle.dump(X_q1_w2v_vect, open(serialization_objects_folder+'X_train_q1_w2v_vect.p','wb'))\n",
    "pickle.dump(X_q2_w2v_vect, open(serialization_objects_folder+'X_train_q2_w2v_vect.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_q1_tfidf, X_q1_w2v_vect, X_q2_tfidf, X_q2_w2v_vect, X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pickle.load(open(serialization_objects_folder+'X_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_all_q = tfidf.transform(tokenize_for_model(model=model_w2v,data_type='test'))\n",
    "# split back into two\n",
    "X_q1_tfidf = X_tfidf_all_q[:len(X_test)]\n",
    "X_q2_tfidf = X_tfidf_all_q[len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((133415, 31889),\n",
       " (133415, 31889),\n",
       " (133415, 4),\n",
       " matrix([[0.41021448, 0.3659877 , 0.61066633, 0.56996817]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X1_q1_tfidf[0] - sparsed vector with float (tfidf)\n",
    "X_q1_tfidf.shape, X_q2_tfidf.shape, X_test.shape, X_q1_tfidf[0,X_q1_tfidf[0].todense().nonzero()[1]].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q1_tfidf, X_q1_w2v_vect = get_weights_and_w2vectors(X_q1_tfidf, tfidf, model_w2v)\n",
    "X_q2_tfidf, X_q2_w2v_vect = get_weights_and_w2vectors(X_q2_tfidf, tfidf, model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "X_q1_tfidf[0].shape, X_q1_w2v_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_q1_tfidf, open(serialization_objects_folder+'X_test_q1_tfidf.p','wb'))\n",
    "pickle.dump(X_q2_tfidf, open(serialization_objects_folder+'X_test_q2_tfidf.p','wb'))\n",
    "pickle.dump(X_q1_w2v_vect, open(serialization_objects_folder+'X_test_q1_w2v_vect.p','wb'))\n",
    "pickle.dump(X_q2_w2v_vect, open(serialization_objects_folder+'X_test_q2_w2v_vect.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.p              X_train.p             readme\r\n",
      "X_test_q1_tfidf.p     X_train_q1_tfidf.p    y_test.p\r\n",
      "X_test_q1_w2v_vect.p  X_train_q1_w2v_vect.p y_train.p\r\n",
      "X_test_q2_tfidf.p     X_train_q2_tfidf.p\r\n",
      "X_test_q2_w2v_vect.p  X_train_q2_w2v_vect.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"$serialization_objects_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_q1_tfidf, X_q1_w2v_vect, X_q2_tfidf, X_q2_w2v_vect, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise Distances & Weighted Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nan_array(r,c):\n",
    "    arr = np.empty((r,c))\n",
    "    arr[:] = np.nan\n",
    "    return arr\n",
    "def compute_pairwise_kernel(pc1, pc2, w1, w2, method='linear'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method=='polynomial':\n",
    "        dist_mat = polynomial_kernel(pc1, pc2, 2)\n",
    "    elif method=='rbf':\n",
    "        dist_mat = rbf_kernel(pc1, pc2)\n",
    "    elif method=='sigmoid':\n",
    "        dist_mat = sigmoid_kernel(pc1, pc2)\n",
    "    elif method=='laplacian':\n",
    "        dist_mat = laplacian_kernel(pc1, pc2)\n",
    "    else:\n",
    "        dist_mat = linear_kernel(pc1, pc2)\n",
    "    return np.average(dist_mat, weights=np.matmul(w1.reshape(-1,1),w2.reshape(-1,1).T))\n",
    "\n",
    "def compute_pairwise_dist(pc1, pc2, w1, w2, method='euclidean'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method=='hausdorff':\n",
    "        dist = directed_hausdorff(pc1, pc2)\n",
    "        return dist[0]\n",
    "    else:\n",
    "        dist_mat = pairwise_distances(pc1, pc2, metric=method) \n",
    "\n",
    "    return np.average(dist_mat, weights=np.matmul(w1.reshape(-1,1),w2.reshape(-1,1).T))\n",
    "\n",
    "def compute_pairwise_for_dataset(X1, X2, X1_w, X2_w, method):\n",
    "    temp = []\n",
    "    for q_tuple in zip(X1, X2, X1_w, X2_w):\n",
    "        if q_tuple:\n",
    "            q1_rd, q2_rd, q1_w, q2_w = q_tuple\n",
    "            if method in ['polynomial', 'rbf', 'sigmoid', 'laplacian', 'linear']:\n",
    "                temp.append(compute_pairwise_kernel(q1_rd, q2_rd, q1_w, q2_w, method))\n",
    "            else:\n",
    "                temp.append(compute_pairwise_dist(q1_rd, q2_rd, q1_w, q2_w, method))\n",
    "        else:\n",
    "            temp.append(np.nan)\n",
    "    return temp\n",
    "\n",
    "def compute_pairwise_for_dataset_wmean(X, X_w, file, store_folder):\n",
    "    temp = []\n",
    "    for q_tuple in zip(X, X_w):\n",
    "        if q_tuple:\n",
    "            q_rd, q_w = q_tuple\n",
    "            if np.sum(q_w) != 0:\n",
    "                temp.append(compute_weighted_mean(q_rd, q_w))\n",
    "            else:\n",
    "                temp.append(create_nan_array(1,300))                    \n",
    "        else:\n",
    "            temp.append(create_nan_array(1,300))\n",
    "    temp_arr = np.array(temp)\n",
    "    pickle_and_remove(temp_arr, file, store_folder) \n",
    "\n",
    "    # computes pairwise metrics, weighted mean and saves to store_folder \n",
    "def compute_and_save(X1, X2, X1_w, X2_w, method, file, store_folder):\n",
    "    computed_obj = compute_pairwise_for_dataset(X1, X2, X1_w, X2_w, method)\n",
    "    pickle_and_remove(computed_obj, file, store_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ['chebyshev','braycurtis', 'cosine', 'correlation', 'canberra', 'hausdorff', 'cityblock',\n",
    "            'euclidean', 'l1', 'l2', 'manhattan', 'minkowski', 'sqeuclidean']\n",
    "\n",
    "def compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder):\n",
    "    name=\"%s_%s_w\"%(distance, data_type)\n",
    "    print(distance)\n",
    "    compute_and_save(X1, X2, X1_w, X2_w, distance, name, store_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_w = pickle.load(open(serialization_objects_folder+'X_train_q1_tfidf.p','rb'))\n",
    "X2_w = pickle.load(open(serialization_objects_folder+'X_train_q2_tfidf.p','rb'))\n",
    "X1 = pickle.load(open(serialization_objects_folder+'X_train_q1_w2v_vect.p','rb'))\n",
    "X2 = pickle.load(open(serialization_objects_folder+'X_train_q2_w2v_vect.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300), (6,), (6, 300))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_w[0].shape,X1[0].shape,X2_w[0].shape,X2[0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chebyshev\n",
      "braycurtis\n",
      "cosine\n",
      "correlation\n",
      "canberra\n",
      "hausdorff\n",
      "cityblock\n",
      "euclidean\n",
      "l1\n",
      "l2\n",
      "manhattan\n",
      "minkowski\n",
      "sqeuclidean\n",
      "sqeuclidean\n",
      "sqeuclidean\n"
     ]
    }
   ],
   "source": [
    "#Run for train\n",
    "data_type = 'train'\n",
    "store_folder = serialization_objects_folder\n",
    "\n",
    "for distance in distances:\n",
    "    compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder)\n",
    "\n",
    "compute_and_save_mean_for_all('weighted_mean1', X1,X1_w, data_type, store_folder)\n",
    "compute_and_save_mean_for_all('weighted_mean2', X2,X2_w, data_type, store_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X1_w, X2_w, X1, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_w = pickle.load(open(serialization_objects_folder+'X_test_q1_tfidf.p','rb'))\n",
    "X2_w = pickle.load(open(serialization_objects_folder+'X_test_q2_tfidf.p','rb'))\n",
    "X1 = pickle.load(open(serialization_objects_folder+'X_test_q1_w2v_vect.p','rb'))\n",
    "X2 = pickle.load(open(serialization_objects_folder+'X_test_q2_w2v_vect.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300), (5,), (5, 300))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_w[0].shape,X1[0].shape,X2_w[0].shape,X2[0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chebyshev\n",
      "braycurtis\n",
      "cosine\n",
      "correlation\n",
      "canberra\n",
      "hausdorff\n",
      "cityblock\n",
      "euclidean\n",
      "l1\n",
      "l2\n",
      "manhattan\n",
      "minkowski\n",
      "sqeuclidean\n",
      "sqeuclidean\n",
      "sqeuclidean\n"
     ]
    }
   ],
   "source": [
    "#Run for train\n",
    "data_type = 'test'\n",
    "store_folder = serialization_objects_folder\n",
    "\n",
    "for distance in distances:\n",
    "    compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_train.p              canberra_train_w.p     l1_train_w.p\r\n",
      "X_test.p               chebyshev_test_w.p     l2_test_w.p\r\n",
      "X_test_q1_tfidf.p      chebyshev_train_w.p    l2_train_w.p\r\n",
      "X_test_q1_w2v_vect.p   cityblock_test_w.p     manhattan_test_w.p\r\n",
      "X_test_q2_tfidf.p      cityblock_train_w.p    manhattan_train_w.p\r\n",
      "X_test_q2_w2v_vect.p   correlation_test_w.p   minkowski_test_w.p\r\n",
      "X_train.p              correlation_train_w.p  minkowski_train_w.p\r\n",
      "X_train_q1_tfidf.p     cosine_test_w.p        readme\r\n",
      "X_train_q1_w2v_vect.p  cosine_train_w.p       sqeuclidean_test_w.p\r\n",
      "X_train_q2_tfidf.p     euclidean_test_w.p     sqeuclidean_train_w.p\r\n",
      "X_train_q2_w2v_vect.p  euclidean_train_w.p    weighted_mean2_train.p\r\n",
      "braycurtis_test_w.p    hausdorff_test_w.p     y_test.p\r\n",
      "braycurtis_train_w.p   hausdorff_train_w.p    y_train.p\r\n",
      "canberra_test_w.p      l1_test_w.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"$serialization_objects_folder\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucu-2019-ml-final-project",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
