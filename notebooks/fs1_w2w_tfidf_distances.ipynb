{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1mGyssgv57T"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Colab needs\n",
    "import os\n",
    "import sys\n",
    "def isCollab():\n",
    "    return os.environ.get('COLAB_GPU', None) != None\n",
    "\n",
    "if isCollab():\n",
    "    #Mounting GDrive disc\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_storage = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "\n",
    "    #Append path where custom modules stored. I put custom modules to GDrive disc\n",
    "    path_to_modules = '/content/gdrive/My Drive/UCU-2019-final-project-storage'\n",
    "    sys.path.append(path_to_modules)\n",
    "else:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pifUtBe1v57J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel, rbf_kernel\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "from fastdtw import fastdtw\n",
    "#import similaritymeasures\n",
    "from scipy.spatial import procrustes\n",
    "from utils.func.functions import pickle_and_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "-vcqtmOMv57Q",
    "outputId": "c00f9a8e-b203-470a-83d1-6941baac656a"
   },
   "outputs": [],
   "source": [
    "if not path_to_storage:\n",
    "    path_to_storage = os.path.abspath(os.path.join(os.getcwd(), '../storage')) \n",
    "\n",
    "data_folder = path_to_storage+'/data/'\n",
    "serialization_objects_folder = path_to_storage+'/serialization_objects/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdpiRIEQv57U"
   },
   "outputs": [],
   "source": [
    "filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]\n",
    "def tokenize(data_type='train'):\n",
    "    if data_type=='test':\n",
    "        X = pickle.load(open(serialization_objects_folder+'X_test.p', 'rb'))\n",
    "    else:\n",
    "        X = pickle.load(open(serialization_objects_folder+'X_train.p', 'rb'))\n",
    "    series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str)\n",
    "    series.dropna()\n",
    "    for question in series:\n",
    "        yield preprocess_string(question, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmK8ZlOtv57X"
   },
   "outputs": [],
   "source": [
    "def tokenize_for_model(model, data_type):\n",
    "    \"\"\"\n",
    "    This function tokenize and check token for dict in model\n",
    "    :param:  model, data_type\n",
    "    :return: ndarray \n",
    "    \"\"\"\n",
    "    for question in tokenize(data_type=data_type):\n",
    "        tf_idf_tokens = []\n",
    "        for token in question:\n",
    "            try:\n",
    "                vector = model.wv[token]\n",
    "                tf_idf_tokens.append(token)\n",
    "            except:\n",
    "                continue\n",
    "        yield np.array(tf_idf_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aqp4rHPNv57Z"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhU1kbHgv57a"
   },
   "source": [
    "#### Embedding: word2vec (Transfer-train on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjhrHi4Yv57b"
   },
   "outputs": [],
   "source": [
    "tokenized_questions = [question for question in tokenize(data_type='train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6b96e2bv57d"
   },
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec(tokenized_questions, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiLbElLRv57g"
   },
   "outputs": [],
   "source": [
    "model_w2v.intersect_word2vec_format(data_folder+'GoogleNews-vectors-negative300.bin',\n",
    "                                lockf=1.0,\n",
    "                                binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUqYRK2Ov57j",
    "outputId": "36d4b2f2-4d61-4f81-f694-5d9614326e07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28526660, 35550940)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(tokenized_questions,total_examples=model_w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M53qGfRwv57s"
   },
   "source": [
    "### Feature Sets 1 - Pairwise Distance & word2vec Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAYEZb5Nv57t"
   },
   "source": [
    "#### Train - TF-IDF Vectorizer and TF-IDF Weights + word2vec Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPwJtvVHv57t"
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(serialization_objects_folder+'X_train.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qgd78u8nv57v"
   },
   "outputs": [],
   "source": [
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "X_tfidf_all_q = tfidf.fit_transform(tokenize_for_model(model=model_w2v,data_type='train'))\n",
    "X_q1_tfidf = X_tfidf_all_q[:len(X_train)]\n",
    "X_q2_tfidf = X_tfidf_all_q[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dw9pddcv57x",
    "outputId": "a9b2b554-5b43-49ff-bd55-9c0ef39a9b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270872, 31889),\n",
       " (270872, 31889),\n",
       " (270872, 4),\n",
       " matrix([[0.56637731, 0.66474651, 0.14063621, 0.46642286]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X1_q1_tfidf[0] - sparsed vector with float (tfidf)\n",
    "X_q1_tfidf.shape, X_q2_tfidf.shape, X_train.shape, X_q1_tfidf[0,X_q1_tfidf[0].todense().nonzero()[1]].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPFilyUlv57z"
   },
   "outputs": [],
   "source": [
    "# function to compute TF-IDF weights as well as the word2vec vectors for all tokens\n",
    "def get_weights_and_w2vectors(tfidf_matrix, tfidf_vectorizer, model):\n",
    "    weights = []\n",
    "    vectors = []\n",
    "    rows = tfidf_matrix.shape[0]\n",
    "    inverse_vocab_dict = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}\n",
    "    for doc in range(rows):\n",
    "        features = tfidf_matrix[doc,:].nonzero()[1]\n",
    "        #weights[i] - all tfidf value for i- row (len(w[i]) - number token/words in row/question)\n",
    "        weights.append(np.array([tfidf_matrix[doc, x] for x in features]))\n",
    "        #vectors[i] - all vectors embeded from model. (len(w[i]) - number token/words in row/question)\n",
    "        vectors.append(np.array([model.wv[inverse_vocab_dict[x]] for x in features]))\n",
    "    return np.array(weights), np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQAia0sDv571"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssb2HtH-v573"
   },
   "source": [
    "##### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0oiDvbWv573"
   },
   "outputs": [],
   "source": [
    "X_q1_tfidf, X_q1_w2v_vect = get_weights_and_w2vectors(X_q1_tfidf, tfidf, model_w2v)\n",
    "X_q2_tfidf, X_q2_w2v_vect = get_weights_and_w2vectors(X_q2_tfidf, tfidf, model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA9LyVJKv575",
    "outputId": "8ca07771-399b-416b-f28a-4b80ac11a42d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "X_q1_tfidf[0].shape, X_q1_w2v_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBEcb0sTv578"
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_q1_tfidf, open(serialization_objects_folder+'X_train_q1_tfidf.p','wb'))\n",
    "pickle.dump(X_q2_tfidf, open(serialization_objects_folder+'X_train_q2_tfidf.p','wb'))\n",
    "pickle.dump(X_q1_w2v_vect, open(serialization_objects_folder+'X_train_q1_w2v_vect.p','wb'))\n",
    "pickle.dump(X_q2_w2v_vect, open(serialization_objects_folder+'X_train_q2_w2v_vect.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ks0opf2v57-"
   },
   "outputs": [],
   "source": [
    "del X_q1_tfidf, X_q1_w2v_vect, X_q2_tfidf, X_q2_w2v_vect, X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GbAxQ6qv58A"
   },
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d_qlPWmv58B"
   },
   "outputs": [],
   "source": [
    "X_test = pickle.load(open(serialization_objects_folder+'X_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWWPMfliv58D"
   },
   "outputs": [],
   "source": [
    "X_tfidf_all_q = tfidf.transform(tokenize_for_model(model=model_w2v,data_type='test'))\n",
    "# split back into two\n",
    "X_q1_tfidf = X_tfidf_all_q[:len(X_test)]\n",
    "X_q2_tfidf = X_tfidf_all_q[len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWVfM01nv58F",
    "outputId": "1d93a509-7296-4e6b-dc1f-aeb3c3d59308"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((133415, 31889),\n",
       " (133415, 31889),\n",
       " (133415, 4),\n",
       " matrix([[0.41021448, 0.3659877 , 0.61066633, 0.56996817]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X1_q1_tfidf[0] - sparsed vector with float (tfidf)\n",
    "X_q1_tfidf.shape, X_q2_tfidf.shape, X_test.shape, X_q1_tfidf[0,X_q1_tfidf[0].todense().nonzero()[1]].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEK04n13v58H"
   },
   "outputs": [],
   "source": [
    "X_q1_tfidf, X_q1_w2v_vect = get_weights_and_w2vectors(X_q1_tfidf, tfidf, model_w2v)\n",
    "X_q2_tfidf, X_q2_w2v_vect = get_weights_and_w2vectors(X_q2_tfidf, tfidf, model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLwq4kg6v58J",
    "outputId": "df9e472b-ffe0-46ea-9613-2fcc540cfb33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "X_q1_tfidf[0].shape, X_q1_w2v_vect[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdDnwU_Wv58L"
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_q1_tfidf, open(serialization_objects_folder+'X_test_q1_tfidf.p','wb'))\n",
    "pickle.dump(X_q2_tfidf, open(serialization_objects_folder+'X_test_q2_tfidf.p','wb'))\n",
    "pickle.dump(X_q1_w2v_vect, open(serialization_objects_folder+'X_test_q1_w2v_vect.p','wb'))\n",
    "pickle.dump(X_q2_w2v_vect, open(serialization_objects_folder+'X_test_q2_w2v_vect.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGsGRioDv58M",
    "outputId": "fc78244c-122e-4036-c176-e8d748aaefbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.p              X_train.p             readme\r\n",
      "X_test_q1_tfidf.p     X_train_q1_tfidf.p    y_test.p\r\n",
      "X_test_q1_w2v_vect.p  X_train_q1_w2v_vect.p y_train.p\r\n",
      "X_test_q2_tfidf.p     X_train_q2_tfidf.p\r\n",
      "X_test_q2_w2v_vect.p  X_train_q2_w2v_vect.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"$serialization_objects_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWRDbxwKv58O"
   },
   "outputs": [],
   "source": [
    "del X_q1_tfidf, X_q1_w2v_vect, X_q2_tfidf, X_q2_w2v_vect, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5xTW4a3v58Q"
   },
   "outputs": [],
   "source": [
    "del model_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zm0dT7cgv58R"
   },
   "source": [
    "#### Pairwise Distances & Weighted Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgEhYIjWv58S"
   },
   "outputs": [],
   "source": [
    "def create_nan_array(r,c):\n",
    "    arr = np.empty((r,c))\n",
    "    arr[:] = np.nan\n",
    "    return arr\n",
    "def compute_pairwise_kernel(pc1, pc2, w1, w2, method='linear'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method=='polynomial':\n",
    "        dist_mat = polynomial_kernel(pc1, pc2, 2)\n",
    "    elif method=='rbf':\n",
    "        dist_mat = rbf_kernel(pc1, pc2)\n",
    "    elif method=='sigmoid':\n",
    "        dist_mat = sigmoid_kernel(pc1, pc2)\n",
    "    elif method=='laplacian':\n",
    "        dist_mat = laplacian_kernel(pc1, pc2)\n",
    "    else:\n",
    "        dist_mat = linear_kernel(pc1, pc2)\n",
    "    return np.average(dist_mat, weights=np.matmul(w1.reshape(-1,1),w2.reshape(-1,1).T))\n",
    "\n",
    "def compute_pairwise_dist(pc1, pc2, w1, w2, method='euclidean'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method=='hausdorff':\n",
    "        dist = directed_hausdorff(pc1, pc2)\n",
    "        return dist[0]\n",
    "    else:\n",
    "        dist_mat = pairwise_distances(pc1, pc2, metric=method) \n",
    "\n",
    "    return np.average(dist_mat, weights=np.matmul(w1.reshape(-1,1),w2.reshape(-1,1).T))\n",
    "\n",
    "def compute_pairwise_for_dataset(X1, X2, X1_w, X2_w, method):\n",
    "    temp = []\n",
    "    for q_tuple in zip(X1, X2, X1_w, X2_w):\n",
    "        if q_tuple:\n",
    "            q1_rd, q2_rd, q1_w, q2_w = q_tuple\n",
    "            if method in ['polynomial', 'rbf', 'sigmoid', 'laplacian', 'linear']:\n",
    "                temp.append(compute_pairwise_kernel(q1_rd, q2_rd, q1_w, q2_w, method))\n",
    "            else:\n",
    "                temp.append(compute_pairwise_dist(q1_rd, q2_rd, q1_w, q2_w, method))\n",
    "        else:\n",
    "            temp.append(np.nan)\n",
    "    return temp\n",
    "\n",
    "def compute_pairwise_for_dataset_wmean(X, X_w, file, store_folder):\n",
    "    temp = []\n",
    "    for q_tuple in zip(X, X_w):\n",
    "        if q_tuple:\n",
    "            q_rd, q_w = q_tuple\n",
    "            if np.sum(q_w) != 0:\n",
    "                temp.append(compute_weighted_mean(q_rd, q_w))\n",
    "            else:\n",
    "                temp.append(create_nan_array(1,300))                    \n",
    "        else:\n",
    "            temp.append(create_nan_array(1,300))\n",
    "    temp_arr = np.array(temp)\n",
    "    pickle_and_remove(temp_arr, file, store_folder) \n",
    "\n",
    "    # computes pairwise metrics, weighted mean and saves to store_folder \n",
    "def compute_and_save(X1, X2, X1_w, X2_w, method, file, store_folder):\n",
    "    computed_obj = compute_pairwise_for_dataset(X1, X2, X1_w, X2_w, method)\n",
    "    pickle_and_remove(computed_obj, file, store_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9km6H4Ymv58U"
   },
   "outputs": [],
   "source": [
    "distances = ['chebyshev','braycurtis', 'cosine', 'correlation', 'canberra', 'hausdorff', 'cityblock',\n",
    "            'euclidean', 'l1', 'l2', 'manhattan', 'minkowski', 'sqeuclidean']\n",
    "\n",
    "def compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder):\n",
    "    name=\"%s_%s_w\"%(distance, data_type)\n",
    "    print(distance)\n",
    "    compute_and_save(X1, X2, X1_w, X2_w, distance, name, store_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p22dK0Gyv58W"
   },
   "source": [
    "##### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okf7Gl_Nv58W"
   },
   "outputs": [],
   "source": [
    "X1_w = pickle.load(open(serialization_objects_folder+'X_train_q1_tfidf.p','rb'))\n",
    "X2_w = pickle.load(open(serialization_objects_folder+'X_train_q2_tfidf.p','rb'))\n",
    "X1 = pickle.load(open(serialization_objects_folder+'X_train_q1_w2v_vect.p','rb'))\n",
    "X2 = pickle.load(open(serialization_objects_folder+'X_train_q2_w2v_vect.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PocZY2brv58Y",
    "outputId": "43bd79cd-4738-42ff-f0de-eef21d0ec6f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300), (6,), (6, 300))"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_w[0].shape,X1[0].shape,X2_w[0].shape,X2[0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HUoM1rCv58a",
    "outputId": "94e530b9-bb1d-4e82-b62b-09f684cd4704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chebyshev\n",
      "braycurtis\n",
      "cosine\n",
      "correlation\n",
      "canberra\n",
      "hausdorff\n",
      "cityblock\n",
      "euclidean\n",
      "l1\n",
      "l2\n",
      "manhattan\n",
      "minkowski\n",
      "sqeuclidean\n",
      "sqeuclidean\n",
      "sqeuclidean\n"
     ]
    }
   ],
   "source": [
    "#Run for train\n",
    "data_type = 'train'\n",
    "store_folder = serialization_objects_folder\n",
    "\n",
    "for distance in distances:\n",
    "    compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder)\n",
    "\n",
    "compute_and_save_mean_for_all('weighted_mean1', X1,X1_w, data_type, store_folder)\n",
    "compute_and_save_mean_for_all('weighted_mean2', X2,X2_w, data_type, store_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F26STY37v58c"
   },
   "outputs": [],
   "source": [
    "del X1_w, X2_w, X1, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIsSmz23v58e"
   },
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNI-J8CUv58e"
   },
   "outputs": [],
   "source": [
    "X1_w = pickle.load(open(serialization_objects_folder+'X_test_q1_tfidf.p','rb'))\n",
    "X2_w = pickle.load(open(serialization_objects_folder+'X_test_q2_tfidf.p','rb'))\n",
    "X1 = pickle.load(open(serialization_objects_folder+'X_test_q1_w2v_vect.p','rb'))\n",
    "X2 = pickle.load(open(serialization_objects_folder+'X_test_q2_w2v_vect.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqSJxkwNv58f",
    "outputId": "136d528b-3d27-4e7e-ddc5-e1c751e7bb22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 300), (5,), (5, 300))"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_w[0].shape,X1[0].shape,X2_w[0].shape,X2[0].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SwlD78Fv58k",
    "outputId": "ec1ea5fc-3672-4559-e326-878724884dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chebyshev\n",
      "braycurtis\n",
      "cosine\n",
      "correlation\n",
      "canberra\n",
      "hausdorff\n",
      "cityblock\n",
      "euclidean\n",
      "l1\n",
      "l2\n",
      "manhattan\n",
      "minkowski\n",
      "sqeuclidean\n",
      "sqeuclidean\n",
      "sqeuclidean\n"
     ]
    }
   ],
   "source": [
    "#Run for train\n",
    "data_type = 'test'\n",
    "store_folder = serialization_objects_folder\n",
    "\n",
    "for distance in distances:\n",
    "    compute_and_save_for_all(X1, X2, X1_w, X2_w, distance, data_type, store_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5FQ4s0jv58m",
    "outputId": "bd801526-bda0-4a3c-e366-6b130e6636ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_train.p              canberra_train_w.p     l1_train_w.p\r\n",
      "X_test.p               chebyshev_test_w.p     l2_test_w.p\r\n",
      "X_test_q1_tfidf.p      chebyshev_train_w.p    l2_train_w.p\r\n",
      "X_test_q1_w2v_vect.p   cityblock_test_w.p     manhattan_test_w.p\r\n",
      "X_test_q2_tfidf.p      cityblock_train_w.p    manhattan_train_w.p\r\n",
      "X_test_q2_w2v_vect.p   correlation_test_w.p   minkowski_test_w.p\r\n",
      "X_train.p              correlation_train_w.p  minkowski_train_w.p\r\n",
      "X_train_q1_tfidf.p     cosine_test_w.p        readme\r\n",
      "X_train_q1_w2v_vect.p  cosine_train_w.p       sqeuclidean_test_w.p\r\n",
      "X_train_q2_tfidf.p     euclidean_test_w.p     sqeuclidean_train_w.p\r\n",
      "X_train_q2_w2v_vect.p  euclidean_train_w.p    weighted_mean2_train.p\r\n",
      "braycurtis_test_w.p    hausdorff_test_w.p     y_test.p\r\n",
      "braycurtis_train_w.p   hausdorff_train_w.p    y_train.p\r\n",
      "canberra_test_w.p      l1_test_w.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"$serialization_objects_folder\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "fs1_w2w_tfidf_distances.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ucu-2019-ml-final-project",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
